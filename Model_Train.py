# -*- coding: utf-8 -*-
"""Training Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WysoGE89V-NKa3OJZnVYi8p0F0npj0R-
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Eye_Project

import warnings
warnings.filterwarnings("ignore")
import cv2,os
data_path='/content/drive/MyDrive/Eye_Project/Dataset'
categories=os.listdir(data_path)
labels=[i for i in range(len(categories))]
label_dict=dict(zip(categories,labels)) #empty dictionary
print(label_dict)
print(categories)
print(labels)

img_size=256
data=[]
target=[]

for category in categories:
  folder_path=os.path.join(data_path,category)
  img_names=os.listdir(folder_path)

  for img_name in img_names:
        img_path=os.path.join(folder_path,img_name)
        img=cv2.imread(img_path)
        
        try:  
            resized=cv2.resize(img,(img_size,img_size))
            #resizing the image  into 100x100, since we need a fixed common size for all the images in the dataset
            data.append(resized)
            target.append(label_dict[category])
            #appending the image and the label(categorized) into the list (dataset)
        except Exception as e:
            print('Exception:',e)
            #if any exception rasied, the exception will be printed here. And pass to the next image

target

import numpy as np
data=np.array(data)/255.0
data=np.reshape(data,(data.shape[0],img_size,img_size,3))
target=np.array(target)
from keras.utils import np_utils
new_target=np_utils.to_categorical(target)

new_target

data.shape

data.shape[1:]

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.models import load_model
 
from keras.constraints import maxnorm  
from keras.layers import Dense,Flatten,GlobalAveragePooling2D,Lambda,Conv2D,MaxPooling2D,Dropout,Activation
from keras.models import Model
from keras.applications.inception_v3 import InceptionV3,preprocess_input

from keras_preprocessing.image import ImageDataGenerator ,load_img,img_to_array
import keras

from keras.models import Sequential
from keras.layers import Dense,Activation,Flatten,Dropout
from keras.layers import Conv2D,MaxPooling2D
from keras.callbacks import ModelCheckpoint

model=Sequential()

model.add(Conv2D(200,(3,3),input_shape=data.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
#The first CNN layer followed by Relu and MaxPooling layers

model.add(Conv2D(100,(3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
#The second convolution layer followed by Relu and MaxPooling layers

model.add(Flatten(input_shape=data.shape[1:]))
model.add(Dropout(0.5))
#Flatten layer to stack the output convolutions from second convolution layer
model.add(Dense(64,activation='relu'))
#Dense layer of 64 neurons
model.add(Dense(128,activation='relu'))
model.add(Dense(256,activation='relu'))
model.add(Dense(7,activation='softmax'))
#The Final layer with two outputs for two categories
  

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

train_datagen = ImageDataGenerator(featurewise_center=True ,
                                   rotation_range= 0.4 , 
                                   width_shift_range=0.3 , 
                                   horizontal_flip= True ,
                                   preprocessing_function= preprocess_input, 
                                   zoom_range=0.4 , 
                                   shear_range=0.4 )

train_data = train_datagen.flow_from_directory('/content/drive/MyDrive/Eye_Project/Dataset' , 
                                               target_size = (256,256) , 
                                             batch_size = 36)

train_data.class_indices

"""# Visualizing the data

"""

t_img , label = train_data.next()

t_img.shape

def plotImages(img_arr , label):
  """
  imput : image arrray
  output : plot images
  """

  for idx , img in enumerate(img_arr):

    if(idx <= 24):

      plt.figure(figsize = (5,5))
      plt.imshow(img)
      plt.title(img.shape)
      plt.axis = False
      plt.show()

plotImages(t_img , label)

"""# Model Check Point"""

from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping

checkpoint = ModelCheckpoint(filepath="./best_model.h5",
                              monitor = "accuracy",
                              verbose =1 , 
                             save_best_only = True )
early = EarlyStopping(monitor = "accuracy" ,
                       min_delta = 0.01 ,                    
                       patience = 5 ,
                       verbose =1)    

cb = [checkpoint,early]

from sklearn.model_selection import train_test_split
train_data,test_data,train_target,test_target=train_test_split(data,new_target,test_size=0.1)

train_data.shape

train_target.shape

hist=model.fit(train_data,train_target,epochs=200,callbacks=cb)

hist.history.keys()

from matplotlib import pyplot as plt

plt.plot(hist.history['accuracy'])
plt.title("ModelTraining Accuracy")
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train'],loc='upper left')
plt.show()

plt.plot(hist.history['loss'])
plt.title("ModelTraining Loss")
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'],loc='upper left')
plt.show()

model_json = model.to_json()
with open("model.json","w") as json_file:
  json_file.write(model_json)

